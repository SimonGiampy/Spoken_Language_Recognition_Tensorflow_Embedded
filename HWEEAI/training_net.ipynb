{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a neural network to classify images of MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nimport os\\n\\n# get parent directory\\nparent_dir = os.path.dirname(os.getcwd())\\n\\n# get the list of csv files in the folder\\ndirectory = parent_dir + \"\\\\datasets\\\\\"\\n\\ncsv_files_ita = [directory + \"\\\\ita\\\\\" + f for f in os.listdir(directory + \"ita\\\\\") if f.endswith(\\'.csv\\')]\\ncsv_files_eng = [directory + \"\\\\eng\\\\\" + f for f in os.listdir(directory + \"eng\\\\\") if f.endswith(\\'.csv\\')]\\n\\nprint(len(csv_files_ita), len(csv_files_eng))\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ON WINDOWS SYSTEM\n",
    "\"\"\"\"\n",
    "import os\n",
    "\n",
    "# get parent directory\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# get the list of csv files in the folder\n",
    "directory = parent_dir + \"\\\\datasets\\\\\"\n",
    "\n",
    "csv_files_ita = [directory + \"\\\\ita\\\\\" + f for f in os.listdir(directory + \"ita\\\\\") if f.endswith('.csv')]\n",
    "csv_files_eng = [directory + \"\\\\eng\\\\\" + f for f in os.listdir(directory + \"eng\\\\\") if f.endswith('.csv')]\n",
    "\n",
    "print(len(csv_files_ita), len(csv_files_eng))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425 450\n"
     ]
    }
   ],
   "source": [
    "#ON WINDOWS SYSTEM\n",
    "\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "directory = os.path.dirname(current_dir) + \"/datasets/\"\n",
    "csv_files_ita = [directory + \"/ita/\" + f for f in os.listdir(directory + \"ita/\") if f.endswith('.csv')]\n",
    "csv_files_eng = [directory + \"/eng/\" + f for f in os.listdir(directory + \"eng/\") if f.endswith('.csv')]\n",
    "\n",
    "print(len(csv_files_ita), len(csv_files_eng))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test import of csv datasets into tensorflow datasets\n",
    "\n",
    "import every csv file as a single matrix with one label associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.4.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    " \n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Check if GPU is available and being used\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data array shape: (1247, 12)\n",
      "(<tf.Tensor: shape=(1247, 12), dtype=int8, numpy=\n",
      "array([[127,   4, -96, ..., -46, -34,  30],\n",
      "       [118,  13, -94, ..., -32, -60,  43],\n",
      "       [108,  30, -94, ...,  -8, -94,  39],\n",
      "       ...,\n",
      "       [-33, -16, -25, ...,  50,  20,   6],\n",
      "       [-30,  12, -37, ...,  39,  29, -31],\n",
      "       [-26,  45, -40, ...,   5,  56, -29]], dtype=int8)>, <tf.Tensor: shape=(), dtype=string, numpy=b'ita'>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:28:24.116032: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-11 17:28:24.118881: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-11 17:28:24.119674: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-11 17:28:24.119737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1650 computeCapability: 7.5\n",
      "coreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\n",
      "2023-08-11 17:28:24.119812: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-11 17:28:24.119927: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-08-11 17:28:24.119975: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-08-11 17:28:24.120000: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-08-11 17:28:24.120025: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-08-11 17:28:24.120047: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-08-11 17:28:24.120069: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-08-11 17:28:24.120093: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-08-11 17:28:24.121111: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-11 17:28:24.130062: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-11 17:28:24.130155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-08-11 17:28:24.130415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-11 17:28:25.931077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-08-11 17:28:25.931146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-08-11 17:28:25.931172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-08-11 17:28:25.932638: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-11 17:28:25.932698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1489] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-11 17:28:25.933428: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-11 17:28:25.934020: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-11 17:28:25.934086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2877 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# read csv file and store the values into a numpy matrix\n",
    "data_array = np.genfromtxt(csv_files_ita[0], delimiter=',', dtype=np.int8)\n",
    "print(\"Loaded data array shape:\", data_array.shape)\n",
    "\n",
    "label = \"ita\"\n",
    "test = [[1, 2], [3, 4]]\n",
    "\n",
    "# Create a TensorFlow dataset\n",
    "tf_dataset_matrix = tf.data.Dataset.from_tensor_slices([data_array])\n",
    "tf_dataset_label = tf.data.Dataset.from_tensor_slices([label])\n",
    "\n",
    "tf_dataset = tf.data.Dataset.zip((tf_dataset_matrix, tf_dataset_label))\n",
    "\n",
    "for element in tf_dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size:  875\n",
      "labels size:  875\n"
     ]
    }
   ],
   "source": [
    "# create empty pandas dataframe\n",
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "# read csv files into dataframe\n",
    "for file in csv_files_ita:\n",
    "    data_array = np.genfromtxt(file, delimiter=',', dtype=np.int8)\n",
    "    dataset.append(data_array)\n",
    "    labels.append(\"ita\")\n",
    "\n",
    "for file in csv_files_eng:\n",
    "    data_array = np.genfromtxt(file, delimiter=',', dtype=np.int8)\n",
    "    dataset.append(data_array)\n",
    "    labels.append(\"eng\")\n",
    "\n",
    "print(\"dataset size: \", len(dataset))\n",
    "print(\"labels size: \", len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "classes = [\"ita\", \"eng\"]\n",
    "\n",
    "# Create a mapping from class names to integer labels\n",
    "class_to_index = {class_name: index for index, class_name in enumerate(classes)}\n",
    "\n",
    "# Convert labels to integer labels using the mapping\n",
    "integer_labels = np.array([class_to_index[label] for label in labels], dtype=np.int8)\n",
    "\n",
    "labels_one_hot = tf.keras.utils.to_categorical(integer_labels, num_classes=2) # one hot encoding\n",
    "\n",
    "print(labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int8\n"
     ]
    }
   ],
   "source": [
    "# train - validation split of tensorflow dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(dataset, labels_one_hot, test_size=0.2, random_state=42) \n",
    "print(val_features[0].dtype)\n",
    "\n",
    "val_features = tf.reshape(val_features, (-1, 1247, 12, 1))\n",
    "train_features = tf.reshape(train_features, (-1, 1247, 12, 1))\n",
    "\n",
    "\n",
    "# create tensorflow dataset from numpy arrays\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_features, val_labels))\n",
    "\n",
    "# shuffle and batch\n",
    "train_dataset = train_dataset.shuffle(len(train_features))\n",
    "\n",
    "val_dataset = val_dataset.batch(32)\n",
    "train_dataset = train_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1247, 12, 1)\n",
      "(32, 2)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_dataset:\n",
    "\tprint(image_batch.shape)\n",
    "\tprint(labels_batch.shape)\n",
    "\tbreak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 1243, 12, 64)      384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 621, 12, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 617, 12, 64)       20544     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 308, 12, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 306, 12, 32)       6176      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 153, 12, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 151, 10, 32)       9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 75, 5, 32)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 38,530\n",
      "Trainable params: 38,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:30:38.059922: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-08-11 17:30:38.069378: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2400005000 Hz\n",
      "2023-08-11 17:30:38.402324: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-08-11 17:30:39.404955: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-08-11 17:30:44.596945: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running ptxas --version returned 256\n",
      "2023-08-11 17:30:44.967317: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 80s 2s/step - loss: 1.0930 - accuracy: 0.5137 - val_loss: 0.6565 - val_accuracy: 0.6914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:31:57.252662: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/claudio/EmbeddedAI/Spoken_Language_Recognition_Tensorflow_Embedded\\models\\/assets\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 2s 72ms/step - loss: 0.6672 - accuracy: 0.6128 - val_loss: 0.6649 - val_accuracy: 0.5371\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.6778 - accuracy: 0.5747 - val_loss: 0.6985 - val_accuracy: 0.5429\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 2s 71ms/step - loss: 0.6824 - accuracy: 0.6034 - val_loss: 0.6517 - val_accuracy: 0.5600\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 2s 71ms/step - loss: 0.7074 - accuracy: 0.5400 - val_loss: 0.7191 - val_accuracy: 0.4914\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.6862 - accuracy: 0.5576 - val_loss: 0.6401 - val_accuracy: 0.6457\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.6410 - accuracy: 0.6140 - val_loss: 0.6068 - val_accuracy: 0.7086\n",
      "INFO:tensorflow:Assets written to: /home/claudio/EmbeddedAI/Spoken_Language_Recognition_Tensorflow_Embedded\\models\\/assets\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.6130 - accuracy: 0.6991 - val_loss: 0.5802 - val_accuracy: 0.7143\n",
      "INFO:tensorflow:Assets written to: /home/claudio/EmbeddedAI/Spoken_Language_Recognition_Tensorflow_Embedded\\models\\/assets\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 2s 72ms/step - loss: 0.6115 - accuracy: 0.6735 - val_loss: 0.5553 - val_accuracy: 0.7371\n",
      "INFO:tensorflow:Assets written to: /home/claudio/EmbeddedAI/Spoken_Language_Recognition_Tensorflow_Embedded\\models\\/assets\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 2s 71ms/step - loss: 0.6079 - accuracy: 0.6672 - val_loss: 0.6853 - val_accuracy: 0.5429\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 2s 73ms/step - loss: 0.6133 - accuracy: 0.6409 - val_loss: 0.5796 - val_accuracy: 0.7371\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 2s 74ms/step - loss: 0.6224 - accuracy: 0.6417 - val_loss: 0.5603 - val_accuracy: 0.7714\n",
      "INFO:tensorflow:Assets written to: /home/claudio/EmbeddedAI/Spoken_Language_Recognition_Tensorflow_Embedded\\models\\/assets\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 2s 71ms/step - loss: 0.5793 - accuracy: 0.6880 - val_loss: 0.5593 - val_accuracy: 0.7371\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 2s 73ms/step - loss: 0.5961 - accuracy: 0.6818 - val_loss: 0.5698 - val_accuracy: 0.7143\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 2s 71ms/step - loss: 0.5630 - accuracy: 0.7051 - val_loss: 0.5363 - val_accuracy: 0.7714\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 2s 71ms/step - loss: 0.5510 - accuracy: 0.7184 - val_loss: 0.5588 - val_accuracy: 0.6914\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 2s 77ms/step - loss: 0.5800 - accuracy: 0.7022 - val_loss: 0.5261 - val_accuracy: 0.7829\n",
      "INFO:tensorflow:Assets written to: /home/claudio/EmbeddedAI/Spoken_Language_Recognition_Tensorflow_Embedded\\models\\/assets\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 2s 73ms/step - loss: 0.5548 - accuracy: 0.7391 - val_loss: 0.5193 - val_accuracy: 0.7314\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 2s 71ms/step - loss: 0.5525 - accuracy: 0.7113 - val_loss: 0.6040 - val_accuracy: 0.6514\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 2s 71ms/step - loss: 0.5754 - accuracy: 0.7075 - val_loss: 0.5121 - val_accuracy: 0.7314\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.5218 - accuracy: 0.7622 - val_loss: 0.6093 - val_accuracy: 0.6400\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.5719 - accuracy: 0.7085 - val_loss: 0.4870 - val_accuracy: 0.8057\n",
      "INFO:tensorflow:Assets written to: /home/claudio/EmbeddedAI/Spoken_Language_Recognition_Tensorflow_Embedded\\models\\/assets\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.4949 - accuracy: 0.7691 - val_loss: 0.4548 - val_accuracy: 0.8000\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.5448 - accuracy: 0.7290 - val_loss: 0.5259 - val_accuracy: 0.7314\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.5313 - accuracy: 0.7215 - val_loss: 0.5113 - val_accuracy: 0.7200\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.5482 - accuracy: 0.7118 - val_loss: 0.4690 - val_accuracy: 0.7657\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.4815 - accuracy: 0.7701 - val_loss: 0.4529 - val_accuracy: 0.8286\n",
      "INFO:tensorflow:Assets written to: /home/claudio/EmbeddedAI/Spoken_Language_Recognition_Tensorflow_Embedded\\models\\/assets\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.5082 - accuracy: 0.7341 - val_loss: 0.4152 - val_accuracy: 0.8171\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 2s 68ms/step - loss: 0.4439 - accuracy: 0.7754 - val_loss: 0.4157 - val_accuracy: 0.8229\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.3832 - accuracy: 0.8351 - val_loss: 0.5354 - val_accuracy: 0.7029\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 2s 68ms/step - loss: 0.4516 - accuracy: 0.7848 - val_loss: 0.7051 - val_accuracy: 0.5143\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.5620 - accuracy: 0.7097 - val_loss: 0.3956 - val_accuracy: 0.7943\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.4873 - accuracy: 0.7845 - val_loss: 0.6690 - val_accuracy: 0.5257\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 1s 67ms/step - loss: 0.4991 - accuracy: 0.7647 - val_loss: 0.3579 - val_accuracy: 0.8686\n",
      "INFO:tensorflow:Assets written to: /home/claudio/EmbeddedAI/Spoken_Language_Recognition_Tensorflow_Embedded\\models\\/assets\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 2s 73ms/step - loss: 0.3615 - accuracy: 0.8501 - val_loss: 0.2966 - val_accuracy: 0.8857\n",
      "INFO:tensorflow:Assets written to: /home/claudio/EmbeddedAI/Spoken_Language_Recognition_Tensorflow_Embedded\\models\\/assets\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.3361 - accuracy: 0.8460 - val_loss: 0.4005 - val_accuracy: 0.8114\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 2s 68ms/step - loss: 0.3536 - accuracy: 0.8440 - val_loss: 0.4972 - val_accuracy: 0.7314\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.3087 - accuracy: 0.8745 - val_loss: 0.2435 - val_accuracy: 0.9029\n",
      "INFO:tensorflow:Assets written to: /home/claudio/EmbeddedAI/Spoken_Language_Recognition_Tensorflow_Embedded\\models\\/assets\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.2911 - accuracy: 0.8828 - val_loss: 0.2615 - val_accuracy: 0.8857\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 2s 68ms/step - loss: 0.2417 - accuracy: 0.9017 - val_loss: 0.3530 - val_accuracy: 0.8400\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.2538 - accuracy: 0.8919 - val_loss: 0.2187 - val_accuracy: 0.9086\n",
      "INFO:tensorflow:Assets written to: /home/claudio/EmbeddedAI/Spoken_Language_Recognition_Tensorflow_Embedded\\models\\/assets\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.1605 - accuracy: 0.9422 - val_loss: 0.1839 - val_accuracy: 0.9086\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.1814 - accuracy: 0.9290 - val_loss: 0.1406 - val_accuracy: 0.9657\n",
      "INFO:tensorflow:Assets written to: /home/claudio/EmbeddedAI/Spoken_Language_Recognition_Tensorflow_Embedded\\models\\/assets\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.1292 - accuracy: 0.9525 - val_loss: 0.4866 - val_accuracy: 0.7829\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.2185 - accuracy: 0.9253 - val_loss: 0.2377 - val_accuracy: 0.8914\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.1390 - accuracy: 0.9527 - val_loss: 0.1233 - val_accuracy: 0.9543\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 2s 71ms/step - loss: 0.1081 - accuracy: 0.9641 - val_loss: 0.1038 - val_accuracy: 0.9771\n",
      "INFO:tensorflow:Assets written to: /home/claudio/EmbeddedAI/Spoken_Language_Recognition_Tensorflow_Embedded\\models\\/assets\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 2s 71ms/step - loss: 0.1176 - accuracy: 0.9572 - val_loss: 0.1239 - val_accuracy: 0.9200\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.1580 - accuracy: 0.9231 - val_loss: 0.1016 - val_accuracy: 0.9600\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.0822 - accuracy: 0.9721 - val_loss: 0.1355 - val_accuracy: 0.9314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe3f8e124c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import absl.logging\n",
    "\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "# Create a basic CNN model\n",
    "model = models.Sequential([\n",
    "    #layers.Reshape(( 1247, 12), input_shape=(1247, 12)),\n",
    "\tlayers.Conv2D(filters=64, kernel_size=(5, 1), activation='relu', input_shape=(1247, 12, 1)),\n",
    "\tlayers.MaxPooling2D(pool_size=(2, 1)),\n",
    "    layers.Conv2D(filters=64, kernel_size=(5, 1), activation='relu'),\n",
    "\tlayers.MaxPooling2D(pool_size=(2, 1)),\n",
    "    layers.Conv2D(filters=32, kernel_size=(3, 1), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 1)),\n",
    "    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.GlobalAveragePooling2D(), \n",
    "\tlayers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "\tlayers.Dense(2, activation='softmax')  # Two classes\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "\t\t\t  loss='categorical_crossentropy',  # Use 'categorical_crossentropy' for one-hot encoded labels\n",
    "\t\t\t  metrics=['accuracy'])\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "filepath = parent_dir + \"\\\\models\\\\\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy',save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=10)\n",
    "\n",
    "callbacks_list = [checkpoint, es]\n",
    "\n",
    "# Train the model\n",
    "model.fit(x=train_dataset, epochs=50, callbacks=callbacks_list, batch_size=32, \n",
    "          validation_data=val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\HP\\Documents\\GitHub\\Spoken_Language_Recognition_Tensorflow_Embedded\\model_lite\\CNN_model_h5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\HP\\Documents\\GitHub\\Spoken_Language_Recognition_Tensorflow_Embedded\\model_lite\\CNN_model_h5\\assets\n"
     ]
    }
   ],
   "source": [
    "filepath = parent_dir + \"\\\\model_lite\\\\\"\n",
    "model.save(filepath +  \"CNN_model_h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(filepath + \"CNN_model\")\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
    "    #tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
    "]\n",
    "\n",
    "converter.experimental_enable_resource_variables = True\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(filepath + \"CNN_model.tflite\", 'wb') as f:\n",
    "\tf.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'serving_default_conv2d_49_input:0', 'index': 0, 'shape': array([   1, 1247,   12,    1]), 'shape_signature': array([  -1, 1247,   12,    1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "[{'name': 'StatefulPartitionedCall:0', 'index': 28, 'shape': array([1, 2]), 'shape_signature': array([-1,  2]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "model_path = filepath + \"CNN_model.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(input_details)\n",
    "print(output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1 1247   12    1]\n",
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "# Assuming single input and output tensors.\n",
    "input_shape = input_details[0]['shape']\n",
    "output_shape = output_details[0]['shape']\n",
    "\n",
    "print(input_shape)\n",
    "print(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-89  32  76 ...  35  63  49]\n",
      " [-84  18  57 ... -11  61  22]\n",
      " [-83  14  66 ... -32  56  20]\n",
      " ...\n",
      " [ -6  87  22 ... -14  14   5]\n",
      " [ 11  71  -4 ...  -7   1 -16]\n",
      " [ 32  69 -33 ... -17   0   2]]\n"
     ]
    }
   ],
   "source": [
    "random_index = np.random.randint(0, len(dataset))\n",
    "\n",
    "print(dataset[0])\n",
    "# Select the random data point using the random index\n",
    "random_data_point = dataset[random_index]\n",
    "random_label = labels_one_hot[random_index]\n",
    "# Convert the random data point from int8 to float32\n",
    "input_data_matrix = random_data_point.astype(np.float32)\n",
    "batch_size = 1\n",
    "\n",
    "input_data_matrix = tf.reshape(input_data_matrix, (-1, 1247, 12, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input data to the interpreter.\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data_matrix)\n",
    "\n",
    "# Run inference.\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output data from the interpreter.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "# Process output data.\n",
    "# For example, if your output is classification probabilities:\n",
    "predicted_class = np.argmax(output_data)\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(random_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environmentAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
