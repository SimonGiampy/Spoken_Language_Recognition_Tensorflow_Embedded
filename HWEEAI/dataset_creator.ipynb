{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create base dataset csv file with columns: language, speaker, audio_raw_data.\n",
    "\n",
    "Loads first the wav files and decodes them into a pandas dataframe. Each audio is approximately 60s long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "audio =  rec_eng_documentary_03.wav (959488,)\n",
      "audio =  rec_fra_interview_8.wav (957696,)\n",
      "audio =  rec_fra_interview_0.wav (960000,)\n",
      "audio =  rec_fra_lessons_02.wav (955136,)\n",
      "audio =  rec_ita_syne_01.wav (957440,)\n",
      "audio =  rec_fra_lessons_01.wav (955136,)\n",
      "audio =  rec_eng_youtube_03.wav (957440,)\n",
      "audio =  rec_eng_lesson_01.wav (957696,)\n",
      "audio =  rec_eng_bourbon_01.wav (957696,)\n",
      "audio =  rec_ita_genitore1_01.wav (957440,)\n",
      "audio =  rec_fra_lorenzo1_04.wav (959488,)\n",
      "audio =  rec_eng_documentary_02.wav (957440,)\n",
      "audio =  rec_fra_interview_2.wav (957696,)\n",
      "audio =  rec_fra_lessons_07.wav (957440,)\n",
      "audio =  rec_ita_genitore2_01.wav (957696,)\n",
      "audio =  rec_eng_documentary_01.wav (957440,)\n",
      "audio =  rec_fra_lessons_04.wav (957696,)\n",
      "audio =  rec_fra_lessons_03.wav (954880,)\n",
      "audio =  rec_eng_lesson_02.wav (957696,)\n",
      "audio =  rec_ita_rita_02.wav (957696,)\n",
      "audio =  rec_fra_interview_7.wav (959744,)\n",
      "audio =  rec_ita_rita_01.wav (957696,)\n",
      "audio =  rec_eng_documentary_05.wav (957696,)\n",
      "audio =  rec_eng_documentary_04.wav (957440,)\n",
      "audio =  rec_fra_lorenzo1_03.wav (957440,)\n",
      "audio =  rec_eng_youtube_02.wav (957696,)\n",
      "audio =  rec_ita_jakidale_01.wav (953600,)\n",
      "audio =  rec_ita_documentary_01.wav (957440,)\n",
      "audio =  rec_fra_lessons_05.wav (957696,)\n",
      "audio =  rec_ita_genitore1_02.wav (957696,)\n",
      "audio =  rec_eng_realscience_02.wav (957440,)\n",
      "audio =  rec_fra_interview_1.wav (957696,)\n",
      "audio =  rec_ita_pdvg_02.wav (955648,)\n",
      "audio =  rec_eng_realscience_01.wav (957184,)\n",
      "audio =  rec_eng_youtube_04.wav (957952,)\n",
      "audio =  rec_eng_youtube_01.wav (957952,)\n",
      "audio =  rec_ita_genitore2_02.wav (957440,)\n",
      "audio =  rec_fra_lessons_06.wav (957440,)\n",
      "audio =  rec_ita_geopop_04.wav (957696,)\n",
      "audio =  rec_ita_pdvg_01.wav (959488,)\n",
      "audio =  rec_ita_documentary_02.wav (949504,)\n",
      "audio =  rec_ita_geopop_03.wav (957696,)\n",
      "['bourbon' 'documentary' 'genitore1' 'genitore2' 'geopop' 'intervie'\n",
      " 'jakidale' 'lesson' 'lessons' 'lorenzo1' 'pdvg' 'realscience' 'rita'\n",
      " 'syne' 'youtube']\n",
      "['eng' 'fra' 'ita']\n",
      "  language      speaker                                     audio_raw_data\n",
      "0      eng  documentary  [-338, -247, -201, -80, 8, -82, -68, -135, -20...\n",
      "1      fra     intervie  [-213, -769, -1007, -1035, -966, -824, -501, -...\n",
      "2      fra     intervie  [-241, -371, -478, -605, -657, -668, -645, -59...\n",
      "3      fra      lessons  [-159, -144, -256, -132, -183, -164, -225, -14...\n",
      "4      ita         syne  [678, 517, 481, 272, 49, -121, -198, -461, -27...\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sample_length = 60 * 16000 # 16kHz sampling of 60 seconds of audio\n",
    "\n",
    "# print size of list of audio files\n",
    "# folder = os.getcwd() + \"/../rec_train/\"\n",
    "# folder = os.getcwd() + \"/../rec_test/test_known_speakers/\"\n",
    "folder = os.getcwd() + \"/../rec_test/test_unheard_speakers/\"\n",
    "\n",
    "# list of wav files in the folder\n",
    "audio_files_list = [audio for audio in os.listdir(folder) if audio.endswith(\".wav\")]\n",
    "print(len(audio_files_list))\n",
    "\n",
    "# memorize audio recordings and thei associated language and speaker\n",
    "# audio_recs = np.zeros( (len(audio_files_list), sample_length), dtype=\"int16\")\n",
    "audio_recs = []\n",
    "languages = []\n",
    "speakers = []\n",
    "\n",
    "# create starting point: dataset with all the audio recordings\n",
    "for i, audio in enumerate(audio_files_list):\n",
    "    sample_rate, audio_rec = wavfile.read(folder + audio)\n",
    "    print(\"audio = \", audio, audio_rec.shape)\n",
    "    audio_recs.append(audio_rec)\n",
    "    languages.append(audio[4:7])\n",
    "    speakers.append(audio[8 : len(audio) - 7])\n",
    "\n",
    "# print unique speakers and languages\n",
    "print(np.unique(speakers))\n",
    "print(np.unique(languages))\n",
    "\n",
    "\n",
    "# Create a dictionary with the structured data\n",
    "data_dictionary = {\n",
    "    'language': languages,\n",
    "    'speaker': speakers,\n",
    "    'audio_raw_data': audio_recs\n",
    "}\n",
    "\n",
    "'''\n",
    "# Specify the data types for each column\n",
    "data_types = {\n",
    "    'audio_raw_data': 'object',\n",
    "    'language': 'str'\n",
    "}\n",
    "'''\n",
    "dataset = pd.DataFrame(data_dictionary)\n",
    "# Save the DataFrame to a CSV file\n",
    "#df.to_csv('dataset0.csv', index=False)\n",
    "\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Validation - Test splitting criteria\n",
    "\n",
    "Create training and validation dataset by splitting the data based on the speaker and language:\n",
    "\n",
    "- all the speakers having 1 or 2 languages associated will have their data split 75-25 between training and validation. Each audio sample is split in a piece of 45 seconds (used for training) and another separate piece of 15 seconds.\n",
    "- speakers having more than 2 languages associated will have 75% of languages in training and 25% of languages in validation. Each audio sample here is taken entirely without splitting, and placed in the corresponding dataset.\n",
    "\n",
    "The data collected are audio samples of 60 seconds. The split between training and validation is done in such a way to have completely separated data frames between the sets. \n",
    "This way the audio frames computed will never overlap between training and validation. This is important in order to have validation data that is unseen in training set. T\n",
    "his is due to the frames of the audio samples computed in sequences of frame_size with some overlap (hop_size). \n",
    "In order to have zero overlap between frames in the 2 datasets, we divide an audio samples in 2 separate non-overlapping pieces, where the frames are computed.\n",
    "\n",
    "This way the validation set can be used to estimate:\n",
    "- known speakers speaking in languages that have been already heard from them\n",
    "- known speakers speaking in languages that were never heard from them -> useful to understand quality of model's knowledge\n",
    "\n",
    "The test set will be created ad-hoc when a lot of data is collected. A few separate test sets will be created, needed to evaluate the performance of the model in different scenarios:\n",
    "\n",
    "1. known speakers in heard languages -> evaluate model performance in tested scenarios\n",
    "2. known speakers in un-heard languages -> evaluate performance for recognizing language instead of the speaker vocal characteristics\n",
    "3. unknown speakers -> evaluate performance for recognizing language from an unseen speaker\n",
    "\n",
    "The forecast for the test task is having the performance in the case 3 being lower than the case 2. Having separate test sets is useful in order to have an unbiased estimate of the model's performance on different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5.6 # seconds of audio in input\n",
    "hop = 1.875 # overlapping window time in seconds\n",
    "frequency = 16000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes as input a series of audio samples, and extracts frames from it without any 75%-25% splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_100(audio_list: pd.DataFrame) -> (pd.DataFrame):\n",
    "    audio_splits = []\n",
    "    languages_test = []\n",
    "    speakers_test = []\n",
    "    \n",
    "    samples = audio_list[\"audio_raw_data\"].to_numpy()\n",
    "    languages = audio_list[\"language\"].to_numpy()\n",
    "    speakers = audio_list[\"speaker\"].to_numpy()\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        \n",
    "        for time in range(0, int((60-window) * frequency), int(hop * frequency)):\n",
    "            audio_splits.append( sample[time : int(time + window * frequency) ].tolist() )\n",
    "            languages_test.append(languages[i])\n",
    "            speakers_test.append(speakers[i])\n",
    "\n",
    "    data_dictionary = {\n",
    "        'language': languages_test,\n",
    "        'speaker': speakers_test,\n",
    "        'audio_raw_data': audio_splits\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data_dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for splitting audio samples in 2 sequences of 45 - 15 seconds each. The frames extracted from the audio samples are well separated and there are no overlapping frames between the 2 splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_75(audio_list: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
    "\taudio_splits_train = []\n",
    "\taudio_splits_val = []\n",
    "\tlanguages_train = []\n",
    "\tlanguages_val = []\n",
    "\tspeakers_train = []\n",
    "\tspeakers_val = []\n",
    "\n",
    "\tsamples = audio_list[\"audio_raw_data\"].to_numpy()\n",
    "\tlanguages = audio_list[\"language\"].to_numpy()\n",
    "\tspeakers = audio_list[\"speaker\"].to_numpy()\n",
    "\n",
    "\tfor i, sample in enumerate(samples):\n",
    "\n",
    "\t\tfor time in range(0, int((45-window) * frequency), int(hop * frequency)):\n",
    "\t\t\taudio_splits_train.append(sample[time : int(time + window * frequency) ].tolist() )\n",
    "\t\t\tlanguages_train.append(languages[i])\n",
    "\t\t\tspeakers_train.append(speakers[i])\n",
    "\t\t\n",
    "\t\tfor time in range(45 * frequency, int((60-window) * frequency), int(hop * frequency)):\n",
    "\t\t\taudio_splits_val.append(sample[time : int(time + window * frequency) ].tolist() )\n",
    "\t\t\tlanguages_val.append(languages[i])\n",
    "\t\t\tspeakers_val.append(speakers[i])\n",
    "\n",
    "\tdata_dictionary_train = {\n",
    "\t\t'language': languages_train,\n",
    "\t\t'speaker': speakers_train,\n",
    "\t\t'audio_raw_data': audio_splits_train\n",
    "\t}\n",
    "\n",
    "\tdata_dictionary_val = {\n",
    "\t\t'language': languages_val,\n",
    "\t\t'speaker': speakers_val,\n",
    "\t\t'audio_raw_data': audio_splits_val\n",
    "\t}\n",
    "\t\n",
    "\t# specify type of data for each column in the dictionaries\n",
    "\tdf_train = pd.DataFrame(data_dictionary_train)\n",
    "\t#df_train = df_train.astype({\"language\": str, \"speaker\": str, \"audio_raw_data\": object})\n",
    "\tdf_val = pd.DataFrame(data_dictionary_val)\n",
    "\t#df_val = df_val.astype({\"language\": str, \"speaker\": str, \"audio_raw_data\": object})\n",
    "\n",
    "\treturn df_train, df_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the list of speakers found in the rec folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bourbon' 'documentary' 'genitore1' 'genitore2' 'geopop' 'intervie'\n",
      " 'jakidale' 'lesson' 'lessons' 'lorenzo1' 'pdvg' 'realscience' 'rita'\n",
      " 'syne' 'youtube']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "split_ratio = 0.75 # 75% of training, 25% of validation\n",
    "\n",
    "classes_list = [\"ita\", \"eng\", \"fra\"] # substitute with np.unique(languages) to obtain whole set of languages\n",
    "speakers_list = np.unique(speakers)\n",
    "print(speakers_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a subset of the languages present in the dataset for training. The resulting dataframe contains only the chosen languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples considered:  42\n",
      "entire dataset:  (42, 3)\n"
     ]
    }
   ],
   "source": [
    "# select the samples that are in dataset_windowed and whose language is in classes_list\n",
    "valid_samples = []\n",
    "for sample in dataset.iterrows():\n",
    "    if sample[1]['language'] in classes_list:\n",
    "        valid_samples.append(sample[1])\n",
    "\n",
    "valid_samples = pd.DataFrame(valid_samples)\n",
    "\n",
    "print(\"samples considered: \", len(valid_samples))\n",
    "print(\"entire dataset: \", dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the dataset into training and validation using the criteria mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker:  adrian  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "speaker:  alessandro  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (5, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "added  110  samples to training\n",
      "added  30  samples to validation\n",
      "speaker:  andrea  speaks:  ['fra']\n",
      "samples quantity:  (1, 3)\n",
      "langs_spoken:  ['fra']\n",
      "added  22  samples to training\n",
      "added  6  samples to validation\n",
      "speaker:  bob  speaks:  ['ita']\n",
      "samples quantity:  (1, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  22  samples to training\n",
      "added  6  samples to validation\n",
      "speaker:  chiara  speaks:  ['eng']\n",
      "samples quantity:  (3, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  66  samples to training\n",
      "added  18  samples to validation\n",
      "speaker:  claudio  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (5, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "added  110  samples to training\n",
      "added  30  samples to validation\n",
      "speaker:  elena  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (7, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "added  154  samples to training\n",
      "added  42  samples to validation\n",
      "speaker:  elisa  speaks:  ['fra']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['fra']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "speaker:  engine  speaks:  ['eng']\n",
      "samples quantity:  (1, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  22  samples to training\n",
      "added  6  samples to validation\n",
      "speaker:  finanza  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "speaker:  francesco  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (4, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "added  88  samples to training\n",
      "added  24  samples to validation\n",
      "speaker:  fratello  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (4, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "added  88  samples to training\n",
      "added  24  samples to validation\n",
      "speaker:  frog  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "speaker:  gabriele  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (4, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "added  88  samples to training\n",
      "added  24  samples to validation\n",
      "speaker:  giopizzi  speaks:  ['ita']\n",
      "samples quantity:  (4, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  88  samples to training\n",
      "added  24  samples to validation\n",
      "speaker:  guia  speaks:  ['fra']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['fra']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "speaker:  hsafari  speaks:  ['ita']\n",
      "samples quantity:  (3, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  66  samples to training\n",
      "added  18  samples to validation\n",
      "speaker:  jet  speaks:  ['eng']\n",
      "samples quantity:  (3, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  66  samples to training\n",
      "added  18  samples to validation\n",
      "speaker:  josephine  speaks:  ['fra']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['fra']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "speaker:  litter  speaks:  ['eng']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "speaker:  lorenzo  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (5, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "added  110  samples to training\n",
      "added  30  samples to validation\n",
      "speaker:  lorenzon  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "speaker:  macellaio  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "speaker:  martina  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (3, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "added  66  samples to training\n",
      "added  18  samples to validation\n",
      "speaker:  mix  speaks:  ['fra']\n",
      "samples quantity:  (41, 3)\n",
      "langs_spoken:  ['fra']\n",
      "added  902  samples to training\n",
      "added  246  samples to validation\n",
      "speaker:  omar  speaks:  ['eng']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "speaker:  paolo  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "speaker:  realeng  speaks:  ['eng']\n",
      "samples quantity:  (4, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  88  samples to training\n",
      "added  24  samples to validation\n",
      "speaker:  simone  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (8, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "added  176  samples to training\n",
      "added  48  samples to validation\n",
      "speaker:  sofia  speaks:  ['eng' 'fra' 'ita']\n",
      "samples quantity:  (9, 3)\n",
      "langs_spoken:  ['eng' 'fra' 'ita']\n",
      "added  198  samples to training\n",
      "added  54  samples to validation\n",
      "speaker:  sushi  speaks:  ['eng']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "speaker:  tanker  speaks:  ['eng']\n",
      "samples quantity:  (3, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  66  samples to training\n",
      "added  18  samples to validation\n",
      "speaker:  terrarium  speaks:  ['eng']\n",
      "samples quantity:  (1, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  22  samples to training\n",
      "added  6  samples to validation\n",
      "speaker:  thiago  speaks:  ['eng']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "speaker:  tiktok  speaks:  ['ita']\n",
      "samples quantity:  (3, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  66  samples to training\n",
      "added  18  samples to validation\n",
      "speaker:  yotobi  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "dataset_train:  3300\n",
      "dataset_validation:  900\n",
      "training ratio:  78.57142857142857 %\n",
      "validation ratio:  21.428571428571427 %\n",
      "total number of samples:  4200\n"
     ]
    }
   ],
   "source": [
    "dataset_train = pd.DataFrame(columns=[\"language\", \"speaker\", \"audio_raw_data\"])\n",
    "dataset_validation = pd.DataFrame(columns=[\"language\", \"speaker\", \"audio_raw_data\"])\n",
    "\n",
    "for speaker in speakers_list:\n",
    "\t# take slice of the dataset containing the samples associated to one speaker\n",
    "\tdata_speaker = valid_samples[ valid_samples[\"speaker\"] == speaker]\n",
    "\tprint(\"speaker: \", speaker, \" speaks: \", np.unique(data_speaker[\"language\"]) )\n",
    "\tprint(\"samples quantity: \", data_speaker.shape)\n",
    "\n",
    "\t# compute number of languages spoken by the speaker\n",
    "\tlangs_spoken = np.unique(data_speaker[\"language\"])\n",
    "\tprint(\"langs_spoken: \", langs_spoken)\n",
    "\tlangs_spoken_count = langs_spoken.shape[0]\n",
    "\t\n",
    "\tsamples_number = len(data_speaker)\n",
    "\n",
    "\n",
    "\t# split an audio sequence of 60 seconds into one sequence of 45s and one sequence of 15s\n",
    "\t# such that none of the frames in the sequences are shared between the two splits\n",
    "\t# the 45s sequence goes in training, the 15s sequence goes in validation\n",
    "\n",
    "\tdata_speaker_train, data_speaker_val = split_75(data_speaker)\n",
    "\t\n",
    "\tcount_train = data_speaker_train.shape[0]\n",
    "\tcount_valid = data_speaker_val.shape[0]\n",
    "\n",
    "\tprint(\"added \", count_train, \" samples to training\")\n",
    "\tprint(\"added \", count_valid, \" samples to validation\")\n",
    "\n",
    "\tdataset_train = pd.concat([dataset_train, data_speaker_train])\n",
    "\tdataset_validation = pd.concat([dataset_validation, data_speaker_val])\n",
    "\n",
    "\n",
    "len_train = len(dataset_train)\n",
    "len_valid = len(dataset_validation)\n",
    "total = len_train + len_valid\n",
    "print(\"dataset_train: \", len_train)\n",
    "print(\"dataset_validation: \", len_valid)\n",
    "print(\"training ratio: \", len_train / total * 100.0, \"%\")\n",
    "print(\"validation ratio: \", len_valid / total * 100.0, \"%\")\n",
    "print(\"total number of samples: \", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the training and validation dataset in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder = os.path.dirname(os.getcwd()) + \"/datasets/\"\n",
    "dataset_train.to_csv(folder + \"dataset_train.csv\", index=False)\n",
    "dataset_validation.to_csv(folder + \"dataset_validation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting csv datasets computed are processed in order to get the MFCCs, which are the actual features of the model.\n",
    "\n",
    "The MFCCs are computed with a cpp script, both at inference time on the arduino and at training time with a linux machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sets computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker:  bourbon  speaks:  ['eng']\n",
      "samples quantity:  (1, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  30  samples to this test set\n",
      "speaker:  documentary  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (7, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "added  210  samples to this test set\n",
      "speaker:  genitore1  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  60  samples to this test set\n",
      "speaker:  genitore2  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  60  samples to this test set\n",
      "speaker:  geopop  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  60  samples to this test set\n",
      "speaker:  intervie  speaks:  ['fra']\n",
      "samples quantity:  (5, 3)\n",
      "langs_spoken:  ['fra']\n",
      "added  150  samples to this test set\n",
      "speaker:  jakidale  speaks:  ['ita']\n",
      "samples quantity:  (1, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  30  samples to this test set\n",
      "speaker:  lesson  speaks:  ['eng']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  60  samples to this test set\n",
      "speaker:  lessons  speaks:  ['fra']\n",
      "samples quantity:  (7, 3)\n",
      "langs_spoken:  ['fra']\n",
      "added  210  samples to this test set\n",
      "speaker:  lorenzo1  speaks:  ['fra']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['fra']\n",
      "added  60  samples to this test set\n",
      "speaker:  pdvg  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  60  samples to this test set\n",
      "speaker:  realscience  speaks:  ['eng']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  60  samples to this test set\n",
      "speaker:  rita  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  60  samples to this test set\n",
      "speaker:  syne  speaks:  ['ita']\n",
      "samples quantity:  (1, 3)\n",
      "langs_spoken:  ['ita']\n",
      "added  30  samples to this test set\n",
      "speaker:  youtube  speaks:  ['eng']\n",
      "samples quantity:  (4, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  120  samples to this test set\n",
      "dataset_test size:  1260\n"
     ]
    }
   ],
   "source": [
    "dataset_test = pd.DataFrame(columns=[\"language\", \"speaker\", \"audio_raw_data\"])\n",
    "\n",
    "for speaker in speakers_list:\n",
    "\t# take slice of the dataset containing the samples associated to one speaker\n",
    "\tdata_speaker = valid_samples[ valid_samples[\"speaker\"] == speaker]\n",
    "\tprint(\"speaker: \", speaker, \" speaks: \", np.unique(data_speaker[\"language\"]) )\n",
    "\tprint(\"samples quantity: \", data_speaker.shape)\n",
    "\n",
    "\t# compute number of languages spoken by the speaker\n",
    "\tlangs_spoken = np.unique(data_speaker[\"language\"])\n",
    "\tprint(\"langs_spoken: \", langs_spoken)\n",
    "\tlangs_spoken_count = langs_spoken.shape[0]\n",
    "\t\n",
    "\tsamples_number = len(data_speaker)\n",
    "\n",
    "\t# this version of the code is meant to compute the test set, so all the generated samples are added\n",
    "\t# to the test set. The splitting is 100% test set and 0% training set\n",
    "\n",
    "\tdata_speaker_test = split_100(data_speaker)\n",
    "\t\n",
    "\tcount_test = data_speaker_test.shape[0]\n",
    "\n",
    "\tprint(\"added \", count_test, \" samples to this test set\")\n",
    "\n",
    "\tdataset_test = pd.concat([dataset_test, data_speaker_test])\n",
    "\n",
    "len_test = len(dataset_test)\n",
    "print(\"dataset_test size: \", len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.dirname(os.getcwd()) + \"/datasets/\"\n",
    "#dataset_test.to_csv(folder + \"dataset_test_known_speakers.csv\", index=False)\n",
    "dataset_test.to_csv(folder + \"dataset_test_unheard_speakers.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
