{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import wav audio samples into a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create base dataset csv file with columns:\n",
    "language, speaker, audio_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "(53, 960000)\n",
      "['alessandro' 'donia' 'elena' 'francesco' 'gabriele' 'lorenzo' 'omar'\n",
      " 'thiago']\n",
      "['ara' 'eng' 'esp' 'ita' 'por']\n",
      "  language     speaker                                     audio_raw_data\n",
      "0      ita  alessandro  [21, -4, -25, -74, -23, -43, -177, -195, -236,...\n",
      "1      por      thiago  [-561, -550, -536, -539, -545, -573, -573, -58...\n",
      "2      ita       elena  [43, 41, 39, 39, 39, 38, 39, 31, 34, 38, 34, 3...\n",
      "3      eng     lorenzo  [-653, -649, -667, -681, -701, -673, -694, -70...\n",
      "4      esp       elena  [-415, -409, -412, -411, -407, -413, -421, -43...\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sample_length = 60 * 16000 # 16kHz sampling of 60 seconds of audio\n",
    "\n",
    "# print size of list of audio files\n",
    "folder = os.getcwd() + \"/../rec/\"\n",
    "audio_files_list = os.listdir(folder)\n",
    "print(len(audio_files_list))\n",
    "\n",
    "# memorize audio recordings and thei associated language and speaker\n",
    "audio_recs = np.zeros( (len(audio_files_list), sample_length), dtype=\"int16\")\n",
    "languages = []\n",
    "speakers = []\n",
    "print(audio_recs.shape)\n",
    "\n",
    "# create starting point: dataset with all the audio recordings\n",
    "for i, audio in enumerate(audio_files_list):\n",
    "    sample_rate, audio_rec = wavfile.read(folder + audio)\n",
    "    audio_recs[i] = audio_rec\n",
    "    languages.append(audio[4:7])\n",
    "    speakers.append(audio[8 : len(audio) - 7])\n",
    "\n",
    "# print unique speakers and languages\n",
    "print(np.unique(speakers))\n",
    "print(np.unique(languages))\n",
    "\n",
    "\n",
    "# Create a dictionary with the structured data\n",
    "data_dictionary = {\n",
    "    'language': languages,\n",
    "    'speaker': speakers,\n",
    "    'audio_raw_data': audio_recs.tolist()\n",
    "}\n",
    "\n",
    "'''\n",
    "# Specify the data types for each column\n",
    "data_types = {\n",
    "    'audio_raw_data': 'object',\n",
    "    'language': 'str'\n",
    "}\n",
    "'''\n",
    "dataset = pd.DataFrame(data_dictionary)\n",
    "# Save the DataFrame to a CSV file\n",
    "#df.to_csv('dataset0.csv', index=False)\n",
    "\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Validation - Test splitting criteria\n",
    "\n",
    "Create training and validation dataset by splitting the data based on the speaker and language:\n",
    "\n",
    "- all the speakers having 1 or 2 languages associated will have their data split 75-25 between training and validation. Each audio sample is split in a piece of 45 seconds (used for training) and another separate piece of 15 seconds.\n",
    "- speakers having more than 2 languages associated will have 75% of languages in training and 25% of languages in validation. Each audio sample here is taken entirely without splitting, and placed in the corresponding dataset.\n",
    "\n",
    "The data collected are audio samples of 60 seconds. The split between training and validation is done in such a way to have completely separated data frames between the sets. \n",
    "This way the audio frames computed will never overlap between training and validation. This is important in order to have validation data that is unseen in training set. T\n",
    "his is due to the frames of the audio samples computed in sequences of frame_size with some overlap (hop_size). \n",
    "In order to have zero overlap between frames in the 2 datasets, we divide an audio samples in 2 separate non-overlapping pieces, where the frames are computed.\n",
    "\n",
    "This way the validation set can be used to estimate:\n",
    "- known speakers speaking in languages that have been already heard from them\n",
    "- known speakers speaking in languages that were never heard from them -> useful to understand quality of model's knowledge\n",
    "\n",
    "The test set will be created ad-hoc when a lot of data is collected. A few separate test sets will be created, needed to evaluate the performance of the model in different scenarios:\n",
    "\n",
    "1. known speakers in heard languages -> evaluate model performance in tested scenarios\n",
    "2. known speakers in un-heard languages -> evaluate performance for recognizing language instead of the speaker vocal characteristics\n",
    "3. unknown speakers -> evaluate performance for recognizing language from an unseen speaker\n",
    "\n",
    "The forecast for the test task is having the performance in the case 3 being lower than the case 2. Having separate test sets is useful in order to have an unbiased estimate of the model's performance on different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5.6 # seconds of audio in input\n",
    "hop = 1.875 # overlapping window time in seconds\n",
    "frequency = 16000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes as input a series of audio samples, and extracts frames from it without any 75%-25% splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_100(audio_list: pd.DataFrame) -> (pd.DataFrame):\n",
    "    audio_splits = []\n",
    "    languages = []\n",
    "    speakers = []\n",
    "    \n",
    "    samples = audio_list[\"audio_raw_data\"].to_numpy()\n",
    "    languages = audio_list[\"language\"].to_numpy()\n",
    "    speakers = audio_list[\"speaker\"].to_numpy()\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        \n",
    "        for time in range(0, int((60-window) * frequency), int(hop * frequency)):\n",
    "            audio_splits.append( sample[time : int(time + window * frequency) ] )\n",
    "            languages.append(languages[i])\n",
    "            speakers.append(speakers[i])\n",
    "\n",
    "    data_dictionary = {\n",
    "        'language': languages,\n",
    "        'speaker': speakers,\n",
    "        'audio_raw_data': audio_splits\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data_dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for splitting audio samples in 2 sequences of 45 - 15 seconds each. The frames extracted from the audio samples are well separated and there are no overlapping frames between the 2 splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_75(audio_list: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
    "    audio_splits_train = []\n",
    "    audio_splits_val = []\n",
    "    languages_train = []\n",
    "    languages_val = []\n",
    "    speakers_train = []\n",
    "    speakers_val = []\n",
    "\n",
    "    samples = audio_list[\"audio_raw_data\"].to_numpy()\n",
    "    languages = audio_list[\"language\"].to_numpy()\n",
    "    speakers = audio_list[\"speaker\"].to_numpy()\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "\n",
    "        for time in range(0, int((45-window) * frequency), int(hop * frequency)):\n",
    "            audio_splits_train.append(sample[time : int(time + window * frequency) ] )\n",
    "            languages_train.append(languages[i])\n",
    "            speakers_train.append(speakers[i])\n",
    "        \n",
    "        for time in range(45 * frequency, int((60-window) * frequency), int(hop * frequency)):\n",
    "            audio_splits_val.append(sample[time : int(time + window * frequency) ] )\n",
    "            languages_val.append(languages[i])\n",
    "            speakers_val.append(speakers[i])\n",
    "\n",
    "    data_dictionary_train = {\n",
    "        'language': languages_train,\n",
    "        'speaker': speakers_train,\n",
    "        'audio_raw_data': audio_splits_train\n",
    "    }\n",
    "\n",
    "    data_dictionary_val = {\n",
    "        'language': languages_val,\n",
    "        'speaker': speakers_val,\n",
    "        'audio_raw_data': audio_splits_val\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data_dictionary_train), pd.DataFrame(data_dictionary_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alessandro' 'donia' 'elena' 'francesco' 'gabriele' 'lorenzo' 'omar'\n",
      " 'thiago']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "split_ratio = 0.75 # 75% of training, 25% of validation\n",
    "\n",
    "classes_list = [\"ita\", \"eng\"] # substitute with np.unique(languages) to obtain whole set of languages\n",
    "speakers_list = np.unique(speakers)\n",
    "print(speakers_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples considered:  35\n",
      "entire dataset:  (53, 3)\n"
     ]
    }
   ],
   "source": [
    "# select the samples that are in dataset_windowed and whose language is in classes_list\n",
    "valid_samples = []\n",
    "for sample in dataset.iterrows():\n",
    "    if sample[1]['language'] in classes_list:\n",
    "        valid_samples.append(sample[1])\n",
    "\n",
    "valid_samples = pd.DataFrame(valid_samples)\n",
    "\n",
    "print(\"samples considered: \", len(valid_samples))\n",
    "print(\"entire dataset: \", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker:  alessandro  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (7, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added  154  samples to training\n",
      "added  42  samples to validation\n",
      "\n",
      "speaker:  donia  speaks:  []\n",
      "samples quantity:  (0, 3)\n",
      "langs_spoken:  []\n",
      "\n",
      "speaker:  elena  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (10, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "added  220  samples to training\n",
      "added  60  samples to validation\n",
      "\n",
      "speaker:  francesco  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (4, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "added  88  samples to training\n",
      "added  24  samples to validation\n",
      "\n",
      "speaker:  gabriele  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (4, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "added  88  samples to training\n",
      "added  24  samples to validation\n",
      "\n",
      "speaker:  lorenzo  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (6, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "added  132  samples to training\n",
      "added  36  samples to validation\n",
      "\n",
      "speaker:  omar  speaks:  ['eng']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "\n",
      "speaker:  thiago  speaks:  ['eng']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['eng']\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "\n",
      "dataset_train:  770\n",
      "dataset_validation:  210\n",
      "training ratio:  78.57142857142857 %\n",
      "validation ratio:  21.428571428571427 %\n",
      "total number of samples:  980\n"
     ]
    }
   ],
   "source": [
    "dataset_train = []\n",
    "dataset_validation = []\n",
    "\n",
    "for speaker in speakers_list:\n",
    "\t# take slice of the dataset containing the samples associated to one speaker\n",
    "\tdata_speaker = valid_samples[ valid_samples[\"speaker\"] == speaker]\n",
    "\tprint(\"speaker: \", speaker, \" speaks: \", np.unique(data_speaker[\"language\"]) )\n",
    "\tprint(\"samples quantity: \", data_speaker.shape)\n",
    "\n",
    "\t# compute number of languages spoken by the speaker\n",
    "\tlangs_spoken = np.unique(data_speaker[\"language\"])\n",
    "\tprint(\"langs_spoken: \", langs_spoken)\n",
    "\tlangs_spoken_count = langs_spoken.shape[0]\n",
    "\t\n",
    "\tsamples_number = len(data_speaker)\n",
    "\n",
    "\tif (langs_spoken_count == 1 or langs_spoken_count == 2):\n",
    "\t\t# split an audio sequence of 60 seconds into one sequence of 45s and one sequence of 15s\n",
    "\t\t# such that none of the frames in the sequences are shared between the two splits\n",
    "\t\t# the 45s sequence goes in training, the 15s sequence goes in validation\n",
    "\n",
    "\t\tdata_speaker_train, data_speaker_val = split_75(data_speaker)\n",
    "\t\t\n",
    "\t\tcount_train = data_speaker_train.shape[0]\n",
    "\t\tcount_valid = data_speaker_val.shape[0]\n",
    "\n",
    "\t\tprint(\"added \", count_train, \" samples to training\")\n",
    "\t\tprint(\"added \", count_valid, \" samples to validation\")\n",
    "\n",
    "\t\tdataset_train.extend(np.array(data_speaker_train))\n",
    "\t\tdataset_validation.extend(np.array(data_speaker_val))\n",
    "\n",
    "\telse: # more than 2 languages spoken by the speaker\n",
    "\t\t# choose 25% of the languages at random spoken by the speaker and place them entirely in validation\n",
    "\t\t# the remaining 75% of the languages are split 75-25 between training and validation\n",
    "\n",
    "\t\tif (langs_spoken_count == 3 or langs_spoken_count == 4):\n",
    "\t\t\t# one language in validation, two in training / validation\n",
    "\t\t\t\n",
    "\t\t\trandom_split = np.random.uniform(0, 1, size=1)\n",
    "\t\t\t# choose the language that goes in validation\n",
    "\t\t\tlang_choice = langs_spoken[int(random_split * langs_spoken_count)]\n",
    "\t\t\tprint(\"chosen language: \", lang_choice, \" to be placed in validation entirely\")\n",
    "\n",
    "\t\t\tfor lang in langs_spoken:\n",
    "\t\t\t\tif (lang == lang_choice):\n",
    "\t\t\t\t\tdata_speaker_val = split_100(data_speaker[ data_speaker[\"language\"] == lang ])\n",
    "\t\t\t\t\tdata_speaker_val = np.array(data_speaker_val)\n",
    "\t\t\t\t\tprint(\"adding \", data_speaker_val.shape[0], \" samples of language \", lang, \" to validation\")\n",
    "\t\t\t\t\tdataset_validation.extend(data_speaker_val)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdata_speaker_train, data_speaker_val = split_75(data_speaker[ data_speaker[\"language\"] == lang ])\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tcount_train = len(data_speaker_train)\n",
    "\t\t\t\t\tcount_valid = len(data_speaker_val)\n",
    "\t\t\t\t\tprint(\"adding \", count_train, \" samples of language \", lang, \" to training\")\n",
    "\t\t\t\t\tprint(\"adding \", count_valid, \" samples of language \", lang, \" to validation\")\n",
    "\n",
    "\t\t\t\t\tdataset_train.extend(np.array(data_speaker_train))\n",
    "\t\t\t\t\tdataset_validation.extend(np.array(data_speaker_val))\n",
    "\n",
    "\t\telse: \n",
    "\t\t\t# if the speaker speaks 5 languages, 2 of them go in validation and 3 in training / validation\n",
    "\t\t\tpass\n",
    "\t\t\n",
    "\tprint(\"\")\n",
    "\n",
    "len_train = len(dataset_train)\n",
    "len_valid = len(dataset_validation)\n",
    "total = len_train + len_valid\n",
    "print(\"dataset_train: \", len_train)\n",
    "print(\"dataset_validation: \", len_valid)\n",
    "print(\"training ratio: \", len_train / total * 100.0, \"%\")\n",
    "print(\"validation ratio: \", len_valid / total * 100.0, \"%\")\n",
    "print(\"total number of samples: \", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the training and validation dataset in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataset_train to dataframe and save it into a csv file\n",
    "# convert dataset_validation to dataframe and save it into a csv file\n",
    "folder = os.path.dirname(os.getcwd()) + \"/datasets/\"\n",
    "\n",
    "train_df = pd.DataFrame(dataset_train)\n",
    "train_df.to_csv(folder + 'dataset_train.csv', index=False)\n",
    "\n",
    "valid_df = pd.DataFrame(dataset_validation)\n",
    "valid_df.to_csv(folder + 'dataset_validation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
