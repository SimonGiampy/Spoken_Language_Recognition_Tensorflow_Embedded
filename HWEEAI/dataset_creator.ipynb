{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create base dataset csv file with columns: language, speaker, audio_raw_data.\n",
    "\n",
    "Loads first the wav files and decodes them into a pandas dataframe. Each audio is approximately 60s long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "audio =  rec_ita_sofia_04.wav (957696,)\n",
      "audio =  rec_ita_alessandro_03.wav (960000,)\n",
      "audio =  rec_por_thiago_01.wav (960000,)\n",
      "audio =  rec_eng_chiara_01.wav (954880,)\n",
      "audio =  rec_ita_simone_02.wav (957440,)\n",
      "audio =  rec_ita_claudio_02.wav (955904,)\n",
      "audio =  rec_eng_simone_05.wav (957440,)\n",
      "audio =  rec_ita_sofia_03.wav (957440,)\n",
      "audio =  rec_ita_elena_03.wav (960000,)\n",
      "audio =  rec_eng_lorenzo_03.wav (960000,)\n",
      "audio =  rec_esp_elena_03.wav (960000,)\n",
      "audio =  rec_esp_elena_05.wav (960000,)\n",
      "audio =  rec_eng_elena_03.wav (960000,)\n",
      "audio =  rec_ita_simone_04.wav (957440,)\n",
      "audio =  rec_ita_elena_02.wav (960000,)\n",
      "audio =  rec_fra_josephine_02.wav (957696,)\n",
      "audio =  rec_por_donia_01.wav (960000,)\n",
      "audio =  rec_fra_guia_03.wav (950528,)\n",
      "audio =  rec_eng_chiara_03.wav (957952,)\n",
      "audio =  rec_ara_omar_03.wav (960000,)\n",
      "audio =  rec_eng_elena_05.wav (960000,)\n",
      "audio =  rec_ita_genitore1_01.wav (957440,)\n",
      "audio =  rec_fra_elisa_01.wav (957440,)\n",
      "audio =  rec_ita_claudio_03.wav (957440,)\n",
      "audio =  rec_eng_claudio_02.wav (957440,)\n",
      "audio =  rec_eng_omar_02.wav (960000,)\n",
      "audio =  rec_ara_omar_02.wav (960000,)\n",
      "audio =  rec_ita_tiktok_03.wav (949248,)\n",
      "audio =  rec_eng_gabriele_01.wav (960000,)\n",
      "audio =  rec_fra_guia_01.wav (957440,)\n",
      "audio =  rec_fra_sofia_02.wav (957696,)\n",
      "audio =  rec_ita_tiktok_02.wav (957696,)\n",
      "audio =  rec_fra_josephine_03.wav (957440,)\n",
      "audio =  rec_ita_fratello_01.wav (956672,)\n",
      "audio =  rec_ita_claudio_01.wav (955904,)\n",
      "audio =  rec_ita_tiktok_01.wav (957696,)\n",
      "audio =  rec_eng_thiago_01.wav (960000,)\n",
      "audio =  rec_por_donia_02.wav (960000,)\n",
      "audio =  rec_ita_alessandro_04.wav (960000,)\n",
      "audio =  rec_eng_thiago_02.wav (960000,)\n",
      "audio =  rec_eng_elena_04.wav (960000,)\n",
      "audio =  rec_ita_simone_03.wav (959744,)\n",
      "audio =  rec_esp_elena_04.wav (960000,)\n",
      "audio =  rec_ita_genitore2_01.wav (957696,)\n",
      "audio =  rec_eng_francesco_02.wav (960000,)\n",
      "audio =  rec_ita_paolo_02.wav (957696,)\n",
      "audio =  rec_eng_omar_01.wav (960000,)\n",
      "audio =  rec_ara_donia_02.wav (960000,)\n",
      "audio =  rec_ita_elena_05.wav (960000,)\n",
      "audio =  rec_ita_rita_02.wav (957696,)\n",
      "audio =  rec_eng_sofia_03.wav (957440,)\n",
      "audio =  rec_esp_martina_01.wav (954880,)\n",
      "audio =  rec_eng_sofia_01.wav (957696,)\n",
      "audio =  rec_ita_paolo_01.wav (957696,)\n",
      "audio =  rec_eng_sofia_02.wav (957696,)\n",
      "audio =  rec_ita_fratello_02.wav (957440,)\n",
      "audio =  rec_eng_claudio_01.wav (957696,)\n",
      "audio =  rec_esp_elena_01.wav (960000,)\n",
      "audio =  rec_ita_rita_01.wav (957696,)\n",
      "audio =  rec_eng_fratello_01.wav (957440,)\n",
      "audio =  rec_por_thiago_03.wav (960000,)\n",
      "audio =  rec_esp_elena_02.wav (960000,)\n",
      "audio =  rec_eng_lorenzo_02.wav (960000,)\n",
      "audio =  rec_ita_elena_04.wav (960000,)\n",
      "audio =  rec_por_donia_03.wav (960000,)\n",
      "audio =  rec_eng_simone_01.wav (960000,)\n",
      "audio =  rec_ita_alessandro_01.wav (960000,)\n",
      "audio =  rec_ita_lorenzo_01.wav (960000,)\n",
      "audio =  rec_eng_simone_02.wav (957696,)\n",
      "audio =  rec_fra_elisa_02.wav (957440,)\n",
      "audio =  rec_ara_donia_01.wav (960000,)\n",
      "audio =  rec_ita_elena_01.wav (960000,)\n",
      "audio =  rec_ita_gabriele_01.wav (960000,)\n",
      "audio =  rec_ita_sofia_05.wav (957696,)\n",
      "audio =  rec_eng_gabriele_02.wav (960000,)\n",
      "audio =  rec_ita_sofia_02.wav (957696,)\n",
      "audio =  rec_eng_alessandro_02.wav (960000,)\n",
      "audio =  rec_eng_alessandro_01.wav (960000,)\n",
      "audio =  rec_por_thiago_02.wav (960000,)\n",
      "audio =  rec_eng_sofia_04.wav (957440,)\n",
      "audio =  rec_ita_genitore1_02.wav (957696,)\n",
      "audio =  rec_eng_simone_03.wav (957696,)\n",
      "audio =  rec_eng_elena_02.wav (960000,)\n",
      "audio =  rec_ara_omar_01.wav (960000,)\n",
      "audio =  rec_por_thiago_04.wav (960000,)\n",
      "audio =  rec_ita_lorenzo_03.wav (960000,)\n",
      "audio =  rec_fra_josephine_01.wav (957440,)\n",
      "audio =  rec_eng_francesco_01.wav (960000,)\n",
      "audio =  rec_eng_lorenzo_01.wav (960000,)\n",
      "audio =  rec_ita_gabriele_02.wav (960000,)\n",
      "audio =  rec_ara_donia_03.wav (960000,)\n",
      "audio =  rec_ita_simone_01.wav (957440,)\n",
      "audio =  rec_eng_fratello_02.wav (957440,)\n",
      "audio =  rec_ita_sofia_01.wav (956928,)\n",
      "audio =  rec_ita_genitore2_02.wav (957440,)\n",
      "audio =  rec_ita_lorenzo_02.wav (960000,)\n",
      "audio =  rec_ita_francesco_01.wav (960000,)\n",
      "audio =  rec_eng_simone_04.wav (957440,)\n",
      "audio =  rec_ita_francesco_02.wav (960000,)\n",
      "audio =  rec_eng_elena_01.wav (960000,)\n",
      "audio =  rec_eng_chiara_02.wav (959488,)\n",
      "audio =  rec_fra_guia_02.wav (957440,)\n",
      "audio =  rec_fra_sofia_01.wav (960000,)\n",
      "audio =  rec_ita_alessandro_02.wav (960000,)\n",
      "audio =  rec_ita_alessandro_05.wav (960000,)\n",
      "['alessandro' 'chiara' 'claudio' 'donia' 'elena' 'elisa' 'francesco'\n",
      " 'fratello' 'gabriele' 'genitore1' 'genitore2' 'guia' 'josephine'\n",
      " 'lorenzo' 'martina' 'omar' 'paolo' 'rita' 'simone' 'sofia' 'thiago'\n",
      " 'tiktok']\n",
      "['ara' 'eng' 'esp' 'fra' 'ita' 'por']\n",
      "  language     speaker                                     audio_raw_data\n",
      "0      ita       sofia  [113, 109, 114, 109, 110, 108, 106, 109, 104, ...\n",
      "1      ita  alessandro  [21, -4, -25, -74, -23, -43, -177, -195, -236,...\n",
      "2      por      thiago  [-561, -550, -536, -539, -545, -573, -573, -58...\n",
      "3      eng      chiara  [-127, -101, -113, -170, -149, -163, -182, -14...\n",
      "4      ita      simone  [343, 332, 344, 352, 336, 347, 356, 350, 355, ...\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sample_length = 60 * 16000 # 16kHz sampling of 60 seconds of audio\n",
    "\n",
    "# print size of list of audio files\n",
    "folder = os.getcwd() + \"/../rec/\"\n",
    "audio_files_list = os.listdir(folder)\n",
    "print(len(audio_files_list))\n",
    "\n",
    "# memorize audio recordings and thei associated language and speaker\n",
    "# audio_recs = np.zeros( (len(audio_files_list), sample_length), dtype=\"int16\")\n",
    "audio_recs = []\n",
    "languages = []\n",
    "speakers = []\n",
    "\n",
    "# create starting point: dataset with all the audio recordings\n",
    "for i, audio in enumerate(audio_files_list):\n",
    "    sample_rate, audio_rec = wavfile.read(folder + audio)\n",
    "    print(\"audio = \", audio, audio_rec.shape)\n",
    "    audio_recs.append(audio_rec)\n",
    "    languages.append(audio[4:7])\n",
    "    speakers.append(audio[8 : len(audio) - 7])\n",
    "\n",
    "# print unique speakers and languages\n",
    "print(np.unique(speakers))\n",
    "print(np.unique(languages))\n",
    "\n",
    "\n",
    "# Create a dictionary with the structured data\n",
    "data_dictionary = {\n",
    "    'language': languages,\n",
    "    'speaker': speakers,\n",
    "    'audio_raw_data': audio_recs\n",
    "}\n",
    "\n",
    "'''\n",
    "# Specify the data types for each column\n",
    "data_types = {\n",
    "    'audio_raw_data': 'object',\n",
    "    'language': 'str'\n",
    "}\n",
    "'''\n",
    "dataset = pd.DataFrame(data_dictionary)\n",
    "# Save the DataFrame to a CSV file\n",
    "#df.to_csv('dataset0.csv', index=False)\n",
    "\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Validation - Test splitting criteria\n",
    "\n",
    "Create training and validation dataset by splitting the data based on the speaker and language:\n",
    "\n",
    "- all the speakers having 1 or 2 languages associated will have their data split 75-25 between training and validation. Each audio sample is split in a piece of 45 seconds (used for training) and another separate piece of 15 seconds.\n",
    "- speakers having more than 2 languages associated will have 75% of languages in training and 25% of languages in validation. Each audio sample here is taken entirely without splitting, and placed in the corresponding dataset.\n",
    "\n",
    "The data collected are audio samples of 60 seconds. The split between training and validation is done in such a way to have completely separated data frames between the sets. \n",
    "This way the audio frames computed will never overlap between training and validation. This is important in order to have validation data that is unseen in training set. T\n",
    "his is due to the frames of the audio samples computed in sequences of frame_size with some overlap (hop_size). \n",
    "In order to have zero overlap between frames in the 2 datasets, we divide an audio samples in 2 separate non-overlapping pieces, where the frames are computed.\n",
    "\n",
    "This way the validation set can be used to estimate:\n",
    "- known speakers speaking in languages that have been already heard from them\n",
    "- known speakers speaking in languages that were never heard from them -> useful to understand quality of model's knowledge\n",
    "\n",
    "The test set will be created ad-hoc when a lot of data is collected. A few separate test sets will be created, needed to evaluate the performance of the model in different scenarios:\n",
    "\n",
    "1. known speakers in heard languages -> evaluate model performance in tested scenarios\n",
    "2. known speakers in un-heard languages -> evaluate performance for recognizing language instead of the speaker vocal characteristics\n",
    "3. unknown speakers -> evaluate performance for recognizing language from an unseen speaker\n",
    "\n",
    "The forecast for the test task is having the performance in the case 3 being lower than the case 2. Having separate test sets is useful in order to have an unbiased estimate of the model's performance on different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5.6 # seconds of audio in input\n",
    "hop = 1.875 # overlapping window time in seconds\n",
    "frequency = 16000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes as input a series of audio samples, and extracts frames from it without any 75%-25% splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_100(audio_list: pd.DataFrame) -> (pd.DataFrame):\n",
    "    audio_splits = []\n",
    "    languages = []\n",
    "    speakers = []\n",
    "    \n",
    "    samples = audio_list[\"audio_raw_data\"].to_numpy()\n",
    "    languages = audio_list[\"language\"].to_numpy()\n",
    "    speakers = audio_list[\"speaker\"].to_numpy()\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        \n",
    "        for time in range(0, int((60-window) * frequency), int(hop * frequency)):\n",
    "            audio_splits.append( sample[time : int(time + window * frequency) ] )\n",
    "            languages.append(languages[i])\n",
    "            speakers.append(speakers[i])\n",
    "\n",
    "    data_dictionary = {\n",
    "        'language': languages,\n",
    "        'speaker': speakers,\n",
    "        'audio_raw_data': audio_splits\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data_dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for splitting audio samples in 2 sequences of 45 - 15 seconds each. The frames extracted from the audio samples are well separated and there are no overlapping frames between the 2 splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_75(audio_list: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
    "\taudio_splits_train = []\n",
    "\taudio_splits_val = []\n",
    "\tlanguages_train = []\n",
    "\tlanguages_val = []\n",
    "\tspeakers_train = []\n",
    "\tspeakers_val = []\n",
    "\n",
    "\tsamples = audio_list[\"audio_raw_data\"].to_numpy()\n",
    "\tlanguages = audio_list[\"language\"].to_numpy()\n",
    "\tspeakers = audio_list[\"speaker\"].to_numpy()\n",
    "\n",
    "\tfor i, sample in enumerate(samples):\n",
    "\n",
    "\t\tfor time in range(0, int((45-window) * frequency), int(hop * frequency)):\n",
    "\t\t\taudio_splits_train.append(sample[time : int(time + window * frequency) ].tolist() )\n",
    "\t\t\tlanguages_train.append(languages[i])\n",
    "\t\t\tspeakers_train.append(speakers[i])\n",
    "\t\t\n",
    "\t\tfor time in range(45 * frequency, int((60-window) * frequency), int(hop * frequency)):\n",
    "\t\t\taudio_splits_val.append(sample[time : int(time + window * frequency) ].tolist() )\n",
    "\t\t\tlanguages_val.append(languages[i])\n",
    "\t\t\tspeakers_val.append(speakers[i])\n",
    "\n",
    "\tdata_dictionary_train = {\n",
    "\t\t'language': languages_train,\n",
    "\t\t'speaker': speakers_train,\n",
    "\t\t'audio_raw_data': audio_splits_train\n",
    "\t}\n",
    "\n",
    "\tdata_dictionary_val = {\n",
    "\t\t'language': languages_val,\n",
    "\t\t'speaker': speakers_val,\n",
    "\t\t'audio_raw_data': audio_splits_val\n",
    "\t}\n",
    "\t\n",
    "\t# specify type of data for each column in the dictionaries\n",
    "\tdf_train = pd.DataFrame(data_dictionary_train)\n",
    "\t#df_train = df_train.astype({\"language\": str, \"speaker\": str, \"audio_raw_data\": object})\n",
    "\tdf_val = pd.DataFrame(data_dictionary_val)\n",
    "\t#df_val = df_val.astype({\"language\": str, \"speaker\": str, \"audio_raw_data\": object})\n",
    "\tprint(df_train.head())\n",
    "\n",
    "\treturn df_train, df_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the list of speakers found in the rec folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alessandro' 'chiara' 'claudio' 'donia' 'elena' 'elisa' 'francesco'\n",
      " 'fratello' 'gabriele' 'genitore1' 'genitore2' 'guia' 'josephine'\n",
      " 'lorenzo' 'martina' 'omar' 'paolo' 'rita' 'simone' 'sofia' 'thiago'\n",
      " 'tiktok']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "split_ratio = 0.75 # 75% of training, 25% of validation\n",
    "\n",
    "classes_list = [\"ita\", \"eng\"] # substitute with np.unique(languages) to obtain whole set of languages\n",
    "speakers_list = np.unique(speakers)\n",
    "print(speakers_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a subset of the languages present in the dataset for training. The resulting dataframe contains only the chosen languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples considered:  76\n",
      "entire dataset:  (105, 3)\n"
     ]
    }
   ],
   "source": [
    "# select the samples that are in dataset_windowed and whose language is in classes_list\n",
    "valid_samples = []\n",
    "for sample in dataset.iterrows():\n",
    "    if sample[1]['language'] in classes_list:\n",
    "        valid_samples.append(sample[1])\n",
    "\n",
    "valid_samples = pd.DataFrame(valid_samples)\n",
    "\n",
    "print(\"samples considered: \", len(valid_samples))\n",
    "print(\"entire dataset: \", dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the dataset into training and validation using the criteria mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker:  alessandro  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (7, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "  language     speaker                                     audio_raw_data\n",
      "0      ita  alessandro  [21, -4, -25, -74, -23, -43, -177, -195, -236,...\n",
      "1      ita  alessandro  [240, 239, 264, 182, 191, 124, 173, 155, 147, ...\n",
      "2      ita  alessandro  [142, 138, 198, 217, 242, 153, 218, 232, 113, ...\n",
      "3      ita  alessandro  [710, 817, 426, 1104, 2481, 3310, 3342, 3349, ...\n",
      "4      ita  alessandro  [1784, 1591, 1904, 2292, 2743, 4057, 4779, 483...\n",
      "added  154  samples to training\n",
      "added  42  samples to validation\n",
      "\n",
      "speaker:  chiara  speaks:  ['eng']\n",
      "samples quantity:  (3, 3)\n",
      "langs_spoken:  ['eng']\n",
      "  language speaker                                     audio_raw_data\n",
      "0      eng  chiara  [-127, -101, -113, -170, -149, -163, -182, -14...\n",
      "1      eng  chiara  [-9, -2, -4, -7, -7, -3, -5, -3, -1, 5, 16, 28...\n",
      "2      eng  chiara  [-39, -68, -121, -133, -102, -55, -48, -125, -...\n",
      "3      eng  chiara  [-113, 6, 71, 123, 163, 184, 188, 263, 386, 39...\n",
      "4      eng  chiara  [256, 247, 234, 224, 227, 238, 237, 205, 179, ...\n",
      "added  66  samples to training\n",
      "added  18  samples to validation\n",
      "\n",
      "speaker:  claudio  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (5, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "  language  speaker                                     audio_raw_data\n",
      "0      ita  claudio  [-185, -201, -193, -203, -227, -199, -230, -21...\n",
      "1      ita  claudio  [178, 211, 223, 185, 186, 180, 169, 70, 67, 50...\n",
      "2      ita  claudio  [-1944, -1582, -1430, -1673, -1806, -1446, -11...\n",
      "3      ita  claudio  [1955, 2081, 2372, 1860, 1118, 1054, 1449, 152...\n",
      "4      ita  claudio  [-137, -180, -214, -279, -88, -134, -276, -32,...\n",
      "added  110  samples to training\n",
      "added  30  samples to validation\n",
      "\n",
      "speaker:  donia  speaks:  []\n",
      "samples quantity:  (0, 3)\n",
      "langs_spoken:  []\n",
      "\n",
      "speaker:  elena  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (10, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "  language speaker                                     audio_raw_data\n",
      "0      ita   elena  [43, 41, 39, 39, 39, 38, 39, 31, 34, 38, 34, 3...\n",
      "1      ita   elena  [4702, 4850, 4372, 3388, 1820, 284, -868, -194...\n",
      "2      ita   elena  [-204, -66, -103, -109, 67, 393, 550, 557, 467...\n",
      "3      ita   elena  [-172, 427, -718, 312, -91, -276, 338, -353, 7...\n",
      "4      ita   elena  [-673, -445, -216, 26, 78, 22, 87, 63, -65, -1...\n",
      "added  220  samples to training\n",
      "added  60  samples to validation\n",
      "\n",
      "speaker:  elisa  speaks:  []\n",
      "samples quantity:  (0, 3)\n",
      "langs_spoken:  []\n",
      "\n",
      "speaker:  francesco  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (4, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "  language    speaker                                     audio_raw_data\n",
      "0      eng  francesco  [-422, -401, -391, -372, -380, -371, -338, -31...\n",
      "1      eng  francesco  [96, 132, 113, 122, 109, 96, 122, 119, 135, 14...\n",
      "2      eng  francesco  [-1070, -1076, -1194, -1252, -1302, -1361, -14...\n",
      "3      eng  francesco  [942, 1042, 1156, 1263, 1318, 1299, 1212, 1112...\n",
      "4      eng  francesco  [-792, -841, -898, -925, -1016, -1117, -1236, ...\n",
      "added  88  samples to training\n",
      "added  24  samples to validation\n",
      "\n",
      "speaker:  fratello  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (4, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "  language   speaker                                     audio_raw_data\n",
      "0      ita  fratello  [547, 555, 554, 549, 546, 547, 541, 537, 535, ...\n",
      "1      ita  fratello  [607, 582, 533, 497, 459, 418, 395, 366, 341, ...\n",
      "2      ita  fratello  [-23, -14, 1, 16, 37, 58, 77, 87, 97, 95, 83, ...\n",
      "3      ita  fratello  [-364, -185, -14, 87, 182, 290, 416, 523, 561,...\n",
      "4      ita  fratello  [462, 515, 527, 502, 434, 350, 288, 255, 229, ...\n",
      "added  88  samples to training\n",
      "added  24  samples to validation\n",
      "\n",
      "speaker:  gabriele  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (4, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "  language   speaker                                     audio_raw_data\n",
      "0      eng  gabriele  [-28, -30, -23, -42, -52, -45, -32, -38, -37, ...\n",
      "1      eng  gabriele  [389, 374, 353, 328, 313, 337, 341, 341, 336, ...\n",
      "2      eng  gabriele  [-304, -4, -255, 45, -181, 164, -159, -16, -69...\n",
      "3      eng  gabriele  [255, 242, 182, 164, 163, 149, 180, 139, 134, ...\n",
      "4      eng  gabriele  [-276, -264, -261, -266, -255, -251, -227, -24...\n",
      "added  88  samples to training\n",
      "added  24  samples to validation\n",
      "\n",
      "speaker:  genitore1  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "  language    speaker                                     audio_raw_data\n",
      "0      ita  genitore1  [328, 323, 311, 308, 312, 310, 296, 293, 298, ...\n",
      "1      ita  genitore1  [191, 203, 205, 211, 219, 212, 195, 199, 191, ...\n",
      "2      ita  genitore1  [1644, 1626, 1587, 1560, 1552, 1549, 1547, 152...\n",
      "3      ita  genitore1  [-737, -710, -692, -703, -711, -686, -657, -62...\n",
      "4      ita  genitore1  [396, 472, 490, 455, 337, 170, 22, -34, -21, 3...\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "\n",
      "speaker:  genitore2  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "  language    speaker                                     audio_raw_data\n",
      "0      ita  genitore2  [-180, -179, -177, -187, -200, -203, -195, -19...\n",
      "1      ita  genitore2  [-672, -555, -357, -199, -44, 141, 330, 549, 7...\n",
      "2      ita  genitore2  [370, 335, 310, 299, 307, 315, 317, 302, 282, ...\n",
      "3      ita  genitore2  [-256, -255, -268, -253, -242, -221, -198, -16...\n",
      "4      ita  genitore2  [88, 92, 94, 98, 102, 103, 100, 100, 92, 90, 1...\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "\n",
      "speaker:  guia  speaks:  []\n",
      "samples quantity:  (0, 3)\n",
      "langs_spoken:  []\n",
      "\n",
      "speaker:  josephine  speaks:  []\n",
      "samples quantity:  (0, 3)\n",
      "langs_spoken:  []\n",
      "\n",
      "speaker:  lorenzo  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (6, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "  language  speaker                                     audio_raw_data\n",
      "0      eng  lorenzo  [-653, -649, -667, -681, -701, -673, -694, -70...\n",
      "1      eng  lorenzo  [31, -12, -10, 49, 88, 92, 113, 146, 184, 207,...\n",
      "2      eng  lorenzo  [2453, 2114, 1579, 982, 459, 30, -329, -738, -...\n",
      "3      eng  lorenzo  [-1049, -1085, -1132, -1187, -1220, -1219, -10...\n",
      "4      eng  lorenzo  [-71, -390, -418, -239, -225, -215, -384, -461...\n",
      "added  132  samples to training\n",
      "added  36  samples to validation\n",
      "\n",
      "speaker:  martina  speaks:  []\n",
      "samples quantity:  (0, 3)\n",
      "langs_spoken:  []\n",
      "\n",
      "speaker:  omar  speaks:  ['eng']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['eng']\n",
      "  language speaker                                     audio_raw_data\n",
      "0      eng    omar  [82, 71, 61, 45, 41, 41, 58, 74, 74, 61, 27, 2...\n",
      "1      eng    omar  [-23, -40, -59, -54, -43, -35, -40, -28, -49, ...\n",
      "2      eng    omar  [-64, -5, 43, -32, -66, 92, 99, -12, -68, -12,...\n",
      "3      eng    omar  [-1378, -1289, -1240, -907, -360, 338, 878, 10...\n",
      "4      eng    omar  [-225, -217, -209, -162, -104, -48, -37, -2, 6...\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "\n",
      "speaker:  paolo  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "  language speaker                                     audio_raw_data\n",
      "0      ita   paolo  [-1386, -1559, -1665, -1766, -1718, -1652, -13...\n",
      "1      ita   paolo  [-302, 60, 222, 385, 526, 715, 876, 1070, 1114...\n",
      "2      ita   paolo  [-3671, -3872, -3758, -3470, -3314, -2891, -22...\n",
      "3      ita   paolo  [-960, -962, -974, -988, -992, -1003, -997, -9...\n",
      "4      ita   paolo  [-31, -28, -30, -33, -32, -34, -46, -45, -53, ...\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "\n",
      "speaker:  rita  speaks:  ['ita']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['ita']\n",
      "  language speaker                                     audio_raw_data\n",
      "0      ita    rita  [505, 497, 499, 503, 512, 515, 519, 528, 532, ...\n",
      "1      ita    rita  [-635, -476, -269, -79, -94, -15, 126, 312, 49...\n",
      "2      ita    rita  [1004, 679, 431, 256, 49, -57, -213, -315, -45...\n",
      "3      ita    rita  [220, 222, 230, 231, 236, 236, 236, 237, 241, ...\n",
      "4      ita    rita  [-97, -342, -208, -60, 144, 280, 366, 520, 687...\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "\n",
      "speaker:  simone  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (9, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "  language speaker                                     audio_raw_data\n",
      "0      ita  simone  [343, 332, 344, 352, 336, 347, 356, 350, 355, ...\n",
      "1      ita  simone  [421, 396, 357, 331, 376, 474, 525, 514, 461, ...\n",
      "2      ita  simone  [-71, -62, -15, 30, 109, 157, 222, 299, 350, 4...\n",
      "3      ita  simone  [-245, -146, 92, 267, 225, 60, -104, -118, -29...\n",
      "4      ita  simone  [-1306, -1339, -1340, -1335, -1401, -1425, -14...\n",
      "added  198  samples to training\n",
      "added  54  samples to validation\n",
      "\n",
      "speaker:  sofia  speaks:  ['eng' 'ita']\n",
      "samples quantity:  (9, 3)\n",
      "langs_spoken:  ['eng' 'ita']\n",
      "  language speaker                                     audio_raw_data\n",
      "0      ita   sofia  [113, 109, 114, 109, 110, 108, 106, 109, 104, ...\n",
      "1      ita   sofia  [81, 303, 400, 502, 606, 708, 798, 897, 979, 9...\n",
      "2      ita   sofia  [-4299, -4392, -4433, -4521, -4624, -4650, -46...\n",
      "3      ita   sofia  [-3169, -2727, -2916, -3314, -4357, -4126, -32...\n",
      "4      ita   sofia  [914, 813, 799, 718, 586, 451, 318, 273, 277, ...\n",
      "added  198  samples to training\n",
      "added  54  samples to validation\n",
      "\n",
      "speaker:  thiago  speaks:  ['eng']\n",
      "samples quantity:  (2, 3)\n",
      "langs_spoken:  ['eng']\n",
      "  language speaker                                     audio_raw_data\n",
      "0      eng  thiago  [73, 47, 59, 57, 31, 43, 26, -4, -26, 9, 16, 5...\n",
      "1      eng  thiago  [-703, -735, -752, -794, -818, -859, -923, -95...\n",
      "2      eng  thiago  [817, 870, 875, 866, 826, 834, 775, 743, 678, ...\n",
      "3      eng  thiago  [62, 118, 109, 90, 144, 106, 133, 144, 124, 12...\n",
      "4      eng  thiago  [356, 586, 1256, 952, -1885, 1399, 274, 636, -...\n",
      "added  44  samples to training\n",
      "added  12  samples to validation\n",
      "\n",
      "speaker:  tiktok  speaks:  ['ita']\n",
      "samples quantity:  (3, 3)\n",
      "langs_spoken:  ['ita']\n",
      "  language speaker                                     audio_raw_data\n",
      "0      ita  tiktok  [353, 350, 346, 348, 346, 344, 338, 340, 342, ...\n",
      "1      ita  tiktok  [79, 91, 105, 93, 52, 21, 19, 77, 102, 133, 95...\n",
      "2      ita  tiktok  [381, 353, 434, 415, 141, -43, -28, -86, -201,...\n",
      "3      ita  tiktok  [-23, -58, -37, -23, -23, -70, -99, -72, -2, -...\n",
      "4      ita  tiktok  [922, 746, 509, 521, 643, 589, 288, -14, -110,...\n",
      "added  66  samples to training\n",
      "added  18  samples to validation\n",
      "\n",
      "dataset_train:  1672\n",
      "dataset_validation:  456\n",
      "training ratio:  78.57142857142857 %\n",
      "validation ratio:  21.428571428571427 %\n",
      "total number of samples:  2128\n"
     ]
    }
   ],
   "source": [
    "dataset_train = pd.DataFrame(columns=[\"language\", \"speaker\", \"audio_raw_data\"])\n",
    "dataset_validation = pd.DataFrame(columns=[\"language\", \"speaker\", \"audio_raw_data\"])\n",
    "\n",
    "for speaker in speakers_list:\n",
    "\t# take slice of the dataset containing the samples associated to one speaker\n",
    "\tdata_speaker = valid_samples[ valid_samples[\"speaker\"] == speaker]\n",
    "\tprint(\"speaker: \", speaker, \" speaks: \", np.unique(data_speaker[\"language\"]) )\n",
    "\tprint(\"samples quantity: \", data_speaker.shape)\n",
    "\n",
    "\t# compute number of languages spoken by the speaker\n",
    "\tlangs_spoken = np.unique(data_speaker[\"language\"])\n",
    "\tprint(\"langs_spoken: \", langs_spoken)\n",
    "\tlangs_spoken_count = langs_spoken.shape[0]\n",
    "\t\n",
    "\tsamples_number = len(data_speaker)\n",
    "\n",
    "\tif (langs_spoken_count == 1 or langs_spoken_count == 2):\n",
    "\t\t# split an audio sequence of 60 seconds into one sequence of 45s and one sequence of 15s\n",
    "\t\t# such that none of the frames in the sequences are shared between the two splits\n",
    "\t\t# the 45s sequence goes in training, the 15s sequence goes in validation\n",
    "\n",
    "\t\tdata_speaker_train, data_speaker_val = split_75(data_speaker)\n",
    "\t\t\n",
    "\t\tcount_train = data_speaker_train.shape[0]\n",
    "\t\tcount_valid = data_speaker_val.shape[0]\n",
    "\n",
    "\t\tprint(\"added \", count_train, \" samples to training\")\n",
    "\t\tprint(\"added \", count_valid, \" samples to validation\")\n",
    "\n",
    "\t\t#dataset_train.extend(np.array(data_speaker_train))\n",
    "\t\t#dataset_validation.extend(np.array(data_speaker_val))\n",
    "\t\tdataset_train = pd.concat([dataset_train, data_speaker_train])\n",
    "\t\tdataset_validation = pd.concat([dataset_validation, data_speaker_val])\n",
    "\n",
    "\telse: # more than 2 languages spoken by the speaker\n",
    "\t\t# choose 25% of the languages at random spoken by the speaker and place them entirely in validation\n",
    "\t\t# the remaining 75% of the languages are split 75-25 between training and validation\n",
    "\n",
    "\t\tif (langs_spoken_count == 3 or langs_spoken_count == 4):\n",
    "\t\t\t# one language in validation, two in training / validation\n",
    "\t\t\t\n",
    "\t\t\trandom_split = np.random.uniform(0, 1, size=1)\n",
    "\t\t\t# choose the language that goes in validation\n",
    "\t\t\tlang_choice = langs_spoken[int(random_split * langs_spoken_count)]\n",
    "\t\t\tprint(\"chosen language: \", lang_choice, \" to be placed in validation entirely\")\n",
    "\n",
    "\t\t\tfor lang in langs_spoken:\n",
    "\t\t\t\tif (lang == lang_choice):\n",
    "\t\t\t\t\tdata_speaker_val = split_100(data_speaker[ data_speaker[\"language\"] == lang ])\n",
    "\t\t\t\t\tdata_speaker_val = np.array(data_speaker_val)\n",
    "\t\t\t\t\tprint(\"adding \", data_speaker_val.shape[0], \" samples of language \", lang, \" to validation\")\n",
    "\t\t\t\t\tdataset_validation.extend(data_speaker_val)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdata_speaker_train, data_speaker_val = split_75(data_speaker[ data_speaker[\"language\"] == lang ])\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tcount_train = len(data_speaker_train)\n",
    "\t\t\t\t\tcount_valid = len(data_speaker_val)\n",
    "\t\t\t\t\tprint(\"adding \", count_train, \" samples of language \", lang, \" to training\")\n",
    "\t\t\t\t\tprint(\"adding \", count_valid, \" samples of language \", lang, \" to validation\")\n",
    "\n",
    "\t\t\t\t\tdataset_train.extend(np.array(data_speaker_train))\n",
    "\t\t\t\t\tdataset_validation.extend(np.array(data_speaker_val))\n",
    "\n",
    "\t\telse: \n",
    "\t\t\t# if the speaker speaks 5 languages, 2 of them go in validation and 3 in training / validation\n",
    "\t\t\tpass\n",
    "\t\t\n",
    "\tprint(\"\")\n",
    "\n",
    "len_train = len(dataset_train)\n",
    "len_valid = len(dataset_validation)\n",
    "total = len_train + len_valid\n",
    "print(\"dataset_train: \", len_train)\n",
    "print(\"dataset_validation: \", len_valid)\n",
    "print(\"training ratio: \", len_train / total * 100.0, \"%\")\n",
    "print(\"validation ratio: \", len_valid / total * 100.0, \"%\")\n",
    "print(\"total number of samples: \", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the training and validation dataset in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder = os.path.dirname(os.getcwd()) + \"/datasets/\"\n",
    "dataset_train.to_csv(folder + \"dataset_train.csv\", index=False)\n",
    "dataset_validation.to_csv(folder + \"dataset_validation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting csv datasets computed are processed in order to get the MFCCs, which are the actual features of the model.\n",
    "\n",
    "The MFCCs are computed with a cpp script, both at inference time on the arduino and at training time with a linux machine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
